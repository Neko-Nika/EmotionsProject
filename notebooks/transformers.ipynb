{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\diploma\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from emotions_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORE_PATH = \"../\"\n",
    "ORIG_PATH = f\"{CORE_PATH}/emotions\"\n",
    "SAVE_LOGS_PATH = f\"{CORE_PATH}/missclassified\"\n",
    "SAVE_MODELS_PATH = f\"{CORE_PATH}/models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 496\n",
       "    Root location: ..//emotions"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = ImageFolder(ORIG_PATH)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anger': 0,\n",
       " 'contempt': 1,\n",
       " 'disgust': 2,\n",
       " 'fear': 3,\n",
       " 'joy': 4,\n",
       " 'sadness': 5,\n",
       " 'wonder': 6}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_to_idx = data.class_to_idx\n",
    "idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "\n",
    "class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(373, 49, 74)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "train_size = 0.75\n",
    "val_size = 0.1\n",
    "test_size = 1 - train_size - val_size\n",
    "\n",
    "train_data, val_data, test_data = torch.utils.data.random_split(\n",
    "    data, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "len(train_data), len(val_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "train_dataset = EmotionsDataset(train_data, train_transforms)\n",
    "val_dataset = EmotionsDataset(val_data, inf_transforms)\n",
    "test_dataset = EmotionsDataset(test_data, inf_transforms)\n",
    "\n",
    "train_loader = get_loader(train_dataset, BATCH_SIZE)\n",
    "val_loader = DataLoader(val_dataset, BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.001\n",
    "EPOCHS = 100\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vit_b_16, ViT_B_16_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vit_b_16(weights=ViT_B_16_Weights.DEFAULT)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "classifier = nn.Sequential(nn.Linear(model.heads[0].in_features, 512),\n",
    "                           nn.ReLU(),\n",
    "                           nn.Dropout(),\n",
    "                           nn.Linear(512, len(class_to_idx)))\n",
    "model.heads = classifier\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/24 [00:00<?, ?it/s]d:\\diploma\\venv\\lib\\site-packages\\torch\\nn\\functional.py:5476: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n",
      "100%|██████████| 24/24 [00:11<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1, train_loss: 1.851833812033523, val_loss: 1.7545764665214383\n",
      "train_f1: 0.24010416666666667, val_f1: 0.21875\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:11<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 2, train_loss: 1.6280841210572394, val_loss: 1.650626785901128\n",
      "train_f1: 0.36354166666666665, val_f1: 0.25\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:10<00:00,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 3, train_loss: 1.5134193500948336, val_loss: 1.6558296096568206\n",
      "train_f1: 0.4145833333333333, val_f1: 0.21875\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:10<00:00,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 4, train_loss: 1.473401653223319, val_loss: 1.5839101149111379\n",
      "train_f1: 0.46458333333333335, val_f1: 0.234375\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:10<00:00,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 5, train_loss: 1.3614791197048115, val_loss: 1.6293716819918886\n",
      "train_f1: 0.47031249999999997, val_f1: 0.234375\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:09<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 6, train_loss: 1.2959421479989632, val_loss: 1.6697646160514987\n",
      "train_f1: 0.5140625, val_f1: 0.25\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:10<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 7, train_loss: 1.2216868931103009, val_loss: 1.506736658057388\n",
      "train_f1: 0.5234375, val_f1: 0.3125\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:10<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 8, train_loss: 1.3069061932550998, val_loss: 1.6221353083240742\n",
      "train_f1: 0.525, val_f1: 0.21875\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:10<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 9, train_loss: 1.154951261770949, val_loss: 1.6984441523649254\n",
      "train_f1: 0.5854166666666667, val_f1: 0.25\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:10<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 10, train_loss: 1.0316877353926446, val_loss: 1.7239181265539052\n",
      "train_f1: 0.6401041666666667, val_f1: 0.296875\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:10<00:00,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 11, train_loss: 1.1250755227602838, val_loss: 1.7271901734021244\n",
      "train_f1: 0.5984375000000001, val_f1: 0.28125\n",
      "\n",
      "Early Stopping!\n"
     ]
    }
   ],
   "source": [
    "loss_train, loss_val = train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    EPOCHS,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    device,\n",
    "    \"best_vit16_100.pt\",\n",
    "    early_stopping=EarlyStopping(3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'..//models/best_vit16_100.pt'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model.state_dict(), f\"{SAVE_MODELS_PATH}/vit16_100_train.pt\")\n",
    "shutil.copyfile(\"best_vit16_100.pt\", f\"{SAVE_MODELS_PATH}/best_vit16_100.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = vit_b_16(weights=ViT_B_16_Weights.DEFAULT)\n",
    "classifier = nn.Sequential(nn.Linear(best.heads[0].in_features, 512),\n",
    "                           nn.ReLU(),\n",
    "                           nn.Dropout(),\n",
    "                           nn.Linear(512, len(class_to_idx)))\n",
    "best.heads = classifier\n",
    "best.load_state_dict(torch.load(f\"{SAVE_MODELS_PATH}/best_vit16_100.pt\"))\n",
    "best.eval()\n",
    "best = best.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31\n"
     ]
    }
   ],
   "source": [
    "test_model(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\diploma\\venv\\lib\\site-packages\\torch\\nn\\functional.py:5476: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27749999999999997\n"
     ]
    }
   ],
   "source": [
    "test_model(best, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Accuracy': 0.3333333333333333,\n",
       " 'Precision_macro': 0.29018174895236365,\n",
       " 'Precision_micro': 0.3333333333333333,\n",
       " 'Recall_macro': 0.2956856677157429,\n",
       " 'Recall_micro': 0.3333333333333333,\n",
       " 'ROC_AUC': {0: 0.6199213630406291,\n",
       "  1: 0.6072874493927126,\n",
       "  2: 0.49557522123893805,\n",
       "  3: 0.486013986013986,\n",
       "  4: 0.7271844660194176,\n",
       "  5: 0.6532653061224489,\n",
       "  6: 0.5513051305130514}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_train_indices = val_data.indices + test_data.indices\n",
    "inf_images = [data.imgs[x][0] for x in not_train_indices]\n",
    "inf_labels = [data.imgs[x][1] for x in not_train_indices]\n",
    "\n",
    "preds = inference_model(best, inf_images, device)\n",
    "get_metrics_report(inf_labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Accuracy': 0.3333333333333333,\n",
       " 'Precision_macro': 0.3358543096872616,\n",
       " 'Precision_micro': 0.3333333333333333,\n",
       " 'Recall_macro': 0.3214825024599461,\n",
       " 'Recall_micro': 0.3333333333333333,\n",
       " 'ROC_AUC': {0: 0.6245085190039318,\n",
       "  1: 0.659919028340081,\n",
       "  2: 0.5190265486725665,\n",
       "  3: 0.5786713286713286,\n",
       "  4: 0.716747572815534,\n",
       "  5: 0.6038775510204082,\n",
       "  6: 0.5335283528352834}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_train_indices = val_data.indices + test_data.indices\n",
    "inf_images = [data.imgs[x][0] for x in not_train_indices]\n",
    "inf_labels = [data.imgs[x][1] for x in not_train_indices]\n",
    "\n",
    "preds = inference_model(model, inf_images, device)\n",
    "get_metrics_report(inf_labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anger emotion\n",
      "Overall images: 14\n",
      "Correctly predicted 4/14\n",
      "\n",
      "contempt emotion\n",
      "Overall images: 19\n",
      "Correctly predicted 7/19\n",
      "\n",
      "disgust emotion\n",
      "Overall images: 10\n",
      "Correctly predicted 0/10\n",
      "\n",
      "fear emotion\n",
      "Overall images: 13\n",
      "Correctly predicted 2/13\n",
      "\n",
      "joy emotion\n",
      "Overall images: 20\n",
      "Correctly predicted 12/20\n",
      "\n",
      "sadness emotion\n",
      "Overall images: 25\n",
      "Correctly predicted 12/25\n",
      "\n",
      "wonder emotion\n",
      "Overall images: 22\n",
      "Correctly predicted 4/22\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_classification_report(inf_labels, preds, idx_to_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_mistaken_images_report(inf_images, inf_labels, preds, \"vit16_100\", idx_to_class, SAVE_LOGS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
